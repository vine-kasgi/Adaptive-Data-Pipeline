2024-07-21 01:58:36,640 - root -INFO -i am in main method..
2024-07-21 01:58:36,641 - root -INFO -calling spark object
2024-07-21 01:58:36,641 - Create_spark -INFO -get_spark_object method started
2024-07-21 01:58:36,642 - Create_spark -INFO -master is local
2024-07-21 01:58:45,382 - Create_spark -INFO -Spark object created...
2024-07-21 01:58:45,382 - root -INFO -validating spark object..............
2024-07-21 01:58:45,383 - Validate -WARNING -started the get_current_date method...
2024-07-21 01:58:56,793 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 01:58:56,794 - Validate -WARNING -Validation done, go frwd...
2024-07-21 01:58:56,794 - root -INFO -Application done
2024-07-21 02:01:41,825 - root -INFO -i am in main method..
2024-07-21 02:01:41,826 - root -INFO -calling spark object
2024-07-21 02:01:41,826 - Create_spark -INFO -get_spark_object method started
2024-07-21 02:01:41,826 - Create_spark -INFO -master is local
2024-07-21 02:01:49,848 - Create_spark -INFO -Spark object created...
2024-07-21 02:01:49,848 - root -INFO -validating spark object..............
2024-07-21 02:01:49,848 - Validate -WARNING -started the get_current_date method...
2024-07-21 02:01:58,956 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 02:01:58,956 - Validate -WARNING -Validation done, go frwd...
2024-07-21 02:01:58,957 - root -INFO -Application done
2024-07-21 19:47:27,090 - root -INFO -i am in main method..
2024-07-21 19:47:27,091 - root -INFO -calling spark object
2024-07-21 19:47:27,091 - Create_spark -INFO -get_spark_object method started
2024-07-21 19:47:27,091 - Create_spark -INFO -master is local
2024-07-21 19:47:49,380 - Create_spark -INFO -Spark object created...
2024-07-21 19:47:49,380 - root -INFO -validating spark object..............
2024-07-21 19:47:49,380 - Validate -WARNING -started the get_current_date method...
2024-07-21 19:47:59,344 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 19:47:59,344 - Validate -WARNING -Validation done, go frwd...
2024-07-21 19:47:59,344 - root -INFO -Application done
2024-07-21 19:52:30,631 - root -INFO -i am in main method..
2024-07-21 19:52:30,632 - root -INFO -calling spark object
2024-07-21 19:52:30,632 - Create_spark -INFO -get_spark_object method started
2024-07-21 19:52:30,632 - Create_spark -INFO -master is local
2024-07-21 19:52:39,158 - Create_spark -INFO -Spark object created...
2024-07-21 19:52:39,158 - root -INFO -validating spark object..............
2024-07-21 19:52:39,158 - Validate -WARNING -started the get_current_date method...
2024-07-21 19:52:48,590 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 19:52:48,590 - Validate -WARNING -Validation done, go frwd...
2024-07-21 19:52:48,590 - root -INFO -Application done
2024-07-21 19:54:58,193 - root -INFO -i am in main method..
2024-07-21 19:54:58,193 - root -INFO -calling spark object
2024-07-21 19:54:58,193 - Create_spark -INFO -get_spark_object method started
2024-07-21 19:54:58,193 - Create_spark -INFO -master is local
2024-07-21 19:55:06,556 - Create_spark -INFO -Spark object created...
2024-07-21 19:55:06,556 - root -INFO -validating spark object..............
2024-07-21 19:55:06,557 - Validate -WARNING -started the get_current_date method...
2024-07-21 19:55:15,403 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 19:55:15,403 - Validate -WARNING -Validation done, go frwd...
2024-07-21 19:55:15,403 - root -INFO -Application done
2024-07-21 19:58:55,398 - root -INFO -i am in main method..
2024-07-21 19:58:55,399 - root -INFO -calling spark object
2024-07-21 19:58:55,399 - Create_spark -INFO -get_spark_object method started
2024-07-21 19:58:55,399 - Create_spark -INFO -master is local
2024-07-21 19:59:02,348 - Create_spark -INFO -Spark object created...
2024-07-21 19:59:02,348 - root -INFO -validating spark object..............
2024-07-21 19:59:02,348 - Validate -WARNING -started the get_current_date method...
2024-07-21 19:59:08,958 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 19:59:08,958 - Validate -WARNING -Validation done, go frwd...
2024-07-21 19:59:08,959 - root -INFO -Application done
2024-07-21 19:59:31,100 - root -INFO -i am in main method..
2024-07-21 19:59:31,101 - root -INFO -calling spark object
2024-07-21 19:59:31,101 - Create_spark -INFO -get_spark_object method started
2024-07-21 19:59:31,101 - Create_spark -INFO -master is local
2024-07-21 19:59:37,854 - Create_spark -INFO -Spark object created...
2024-07-21 19:59:37,854 - root -INFO -validating spark object..............
2024-07-21 19:59:37,854 - Validate -WARNING -started the get_current_date method...
2024-07-21 19:59:44,683 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 19:59:44,683 - Validate -WARNING -Validation done, go frwd...
2024-07-21 19:59:44,684 - root -INFO -Application done
2024-07-21 20:12:36,952 - root -INFO -i am in main method..
2024-07-21 20:12:36,952 - root -INFO -calling spark object
2024-07-21 20:12:36,952 - Create_spark -INFO -get_spark_object method started
2024-07-21 20:12:36,952 - Create_spark -INFO -master is local
2024-07-21 20:12:45,254 - Create_spark -INFO -Spark object created...
2024-07-21 20:12:45,254 - root -INFO -validating spark object..............
2024-07-21 20:12:45,254 - Validate -WARNING -started the get_current_date method...
2024-07-21 20:12:53,826 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 20:12:53,826 - Validate -WARNING -Validation done, go frwd...
2024-07-21 20:12:53,826 - root -INFO -reading file which is of = parquet
2024-07-21 20:12:53,827 - root -INFO -Application done
2024-07-21 20:13:32,721 - root -INFO -i am in main method..
2024-07-21 20:13:32,721 - root -INFO -calling spark object
2024-07-21 20:13:32,722 - Create_spark -INFO -get_spark_object method started
2024-07-21 20:13:32,722 - Create_spark -INFO -master is local
2024-07-21 20:13:41,566 - Create_spark -INFO -Spark object created...
2024-07-21 20:13:41,566 - root -INFO -validating spark object..............
2024-07-21 20:13:41,566 - Validate -WARNING -started the get_current_date method...
2024-07-21 20:13:50,347 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 20:13:50,347 - Validate -WARNING -Validation done, go frwd...
2024-07-21 20:13:50,347 - root -INFO -reading file which is of = parquet
2024-07-21 20:13:50,348 - root -INFO -Application done
2024-07-21 20:52:17,920 - root -INFO -i am in main method..
2024-07-21 20:52:17,920 - root -INFO -calling spark object
2024-07-21 20:52:17,921 - Create_spark -INFO -get_spark_object method started
2024-07-21 20:52:17,921 - Create_spark -INFO -master is local
2024-07-21 20:52:35,112 - Create_spark -INFO -Spark object created...
2024-07-21 20:52:35,112 - root -INFO -validating spark object..............
2024-07-21 20:52:35,112 - Validate -WARNING -started the get_current_date method...
2024-07-21 20:52:41,771 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 20:52:41,772 - Validate -WARNING -Validation done, go frwd...
2024-07-21 20:52:41,772 - root -INFO -reading file which is of = parquet
2024-07-21 20:52:41,772 - Ingest -WARNING -load_files method started....
2024-07-21 20:52:42,948 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-07-21 20:52:42,954 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 20:52:44,474 - root -INFO -Application done
2024-07-21 21:01:06,684 - root -INFO -i am in main method..
2024-07-21 21:01:06,685 - root -INFO -calling spark object
2024-07-21 21:01:06,685 - Create_spark -INFO -get_spark_object method started
2024-07-21 21:01:06,685 - Create_spark -INFO -master is local
2024-07-21 21:01:15,497 - Create_spark -INFO -Spark object created...
2024-07-21 21:01:15,497 - root -INFO -validating spark object..............
2024-07-21 21:01:15,498 - Validate -WARNING -started the get_current_date method...
2024-07-21 21:01:24,827 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 21:01:24,827 - Validate -WARNING -Validation done, go frwd...
2024-07-21 21:01:24,827 - root -INFO -reading file which is of = parquet
2024-07-21 21:01:24,828 - Ingest -WARNING -load_files method started....
2024-07-21 21:01:26,007 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-07-21 21:01:26,014 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 21:01:27,811 - root -INFO -validating the dataframe...
2024-07-21 21:01:27,812 - Ingest -WARNING -here to count the records in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 21:01:29,072 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-21 21:01:29,073 - root -INFO -Application done
2024-07-21 21:04:02,682 - root -INFO -i am in main method..
2024-07-21 21:04:02,683 - root -INFO -calling spark object
2024-07-21 21:04:02,683 - Create_spark -INFO -get_spark_object method started
2024-07-21 21:04:02,683 - Create_spark -INFO -master is local
2024-07-21 21:04:10,196 - Create_spark -INFO -Spark object created...
2024-07-21 21:04:10,196 - root -INFO -validating spark object..............
2024-07-21 21:04:10,196 - Validate -WARNING -started the get_current_date method...
2024-07-21 21:04:17,821 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 21:04:17,821 - Validate -WARNING -Validation done, go frwd...
2024-07-21 21:04:17,822 - root -INFO -reading file which is of = parquet
2024-07-21 21:04:17,822 - Ingest -WARNING -load_files method started....
2024-07-21 21:04:18,855 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-07-21 21:04:18,866 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 21:04:20,767 - root -INFO -validating the dataframe...
2024-07-21 21:04:20,767 - Ingest -WARNING -here to count the records in the df_city
2024-07-21 21:04:22,149 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-21 21:04:22,149 - root -INFO -Application done
2024-07-21 21:11:38,390 - root -INFO -i am in main method..
2024-07-21 21:11:38,390 - root -INFO -calling spark object
2024-07-21 21:11:38,390 - Create_spark -INFO -get_spark_object method started
2024-07-21 21:11:38,390 - Create_spark -INFO -master is local
2024-07-21 21:11:47,311 - Create_spark -INFO -Spark object created...
2024-07-21 21:11:47,312 - root -INFO -validating spark object..............
2024-07-21 21:11:47,312 - Validate -WARNING -started the get_current_date method...
2024-07-21 21:11:56,101 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 21:11:56,101 - Validate -WARNING -Validation done, go frwd...
2024-07-21 21:11:56,102 - root -INFO -reading file which is of = csv
2024-07-21 21:11:56,102 - Ingest -WARNING -load_files method started....
2024-07-21 21:15:05,529 - root -INFO -i am in main method..
2024-07-21 21:15:05,529 - root -INFO -calling spark object
2024-07-21 21:15:05,529 - Create_spark -INFO -get_spark_object method started
2024-07-21 21:15:05,530 - Create_spark -INFO -master is local
2024-07-21 21:15:12,463 - Create_spark -INFO -Spark object created...
2024-07-21 21:15:12,463 - root -INFO -validating spark object..............
2024-07-21 21:15:12,463 - Validate -WARNING -started the get_current_date method...
2024-07-21 21:15:19,530 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 21:15:19,530 - Validate -WARNING -Validation done, go frwd...
2024-07-21 21:15:19,531 - root -INFO -reading file which is of = csv
2024-07-21 21:15:19,531 - Ingest -WARNING -load_files method started....
2024-07-21 21:18:42,735 - root -INFO -i am in main method..
2024-07-21 21:18:42,735 - root -INFO -calling spark object
2024-07-21 21:18:42,735 - Create_spark -INFO -get_spark_object method started
2024-07-21 21:18:42,735 - Create_spark -INFO -master is local
2024-07-21 21:18:50,951 - Create_spark -INFO -Spark object created...
2024-07-21 21:18:50,952 - root -INFO -validating spark object..............
2024-07-21 21:18:50,952 - Validate -WARNING -started the get_current_date method...
2024-07-21 21:18:58,481 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 21:18:58,481 - Validate -WARNING -Validation done, go frwd...
2024-07-21 21:18:58,482 - root -INFO -reading file which is of = parquet
2024-07-21 21:18:58,482 - Ingest -WARNING -load_files method started....
2024-07-21 21:18:59,817 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-07-21 21:18:59,829 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 21:19:01,380 - root -INFO -validating the dataframe...
2024-07-21 21:19:01,380 - Ingest -WARNING -here to count the records in the df_city
2024-07-21 21:19:02,421 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-21 21:19:02,421 - root -INFO -reading file which is of = csv
2024-07-21 21:19:02,422 - Ingest -WARNING -load_files method started....
2024-07-21 21:28:51,838 - root -INFO -i am in main method..
2024-07-21 21:28:51,838 - root -INFO -calling spark object
2024-07-21 21:28:51,838 - Create_spark -INFO -get_spark_object method started
2024-07-21 21:28:51,839 - Create_spark -INFO -master is local
2024-07-21 21:28:59,093 - Create_spark -INFO -Spark object created...
2024-07-21 21:28:59,093 - root -INFO -validating spark object..............
2024-07-21 21:28:59,093 - Validate -WARNING -started the get_current_date method...
2024-07-21 21:29:06,435 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 21:29:06,435 - Validate -WARNING -Validation done, go frwd...
2024-07-21 21:29:06,436 - root -INFO -reading file which is of = parquet
2024-07-21 21:29:06,436 - Ingest -WARNING -load_files method started....
2024-07-21 21:29:07,415 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-07-21 21:29:07,422 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 21:29:08,760 - root -INFO -validating the dataframe...
2024-07-21 21:29:08,760 - Ingest -WARNING -here to count the records in the df_city
2024-07-21 21:29:09,726 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-21 21:29:09,727 - root -INFO -reading file which is of = csv
2024-07-21 21:29:09,727 - Ingest -WARNING -load_files method started....
2024-07-21 21:32:54,154 - root -INFO -i am in main method..
2024-07-21 21:32:54,154 - root -INFO -calling spark object
2024-07-21 21:32:54,154 - Create_spark -INFO -get_spark_object method started
2024-07-21 21:32:54,154 - Create_spark -INFO -master is local
2024-07-21 21:33:00,804 - Create_spark -INFO -Spark object created...
2024-07-21 21:33:00,804 - root -INFO -validating spark object..............
2024-07-21 21:33:00,804 - Validate -WARNING -started the get_current_date method...
2024-07-21 21:33:07,835 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 21:33:07,835 - Validate -WARNING -Validation done, go frwd...
2024-07-21 21:33:07,835 - root -INFO -reading file which is of = parquet
2024-07-21 21:33:07,835 - Ingest -WARNING -load_files method started....
2024-07-21 21:33:08,985 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-07-21 21:33:08,993 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 21:33:10,615 - root -INFO -validating the dataframe...
2024-07-21 21:33:10,615 - Ingest -WARNING -here to count the records in the df_city
2024-07-21 21:33:12,035 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-21 21:33:12,035 - root -INFO -reading file which is of = csv
2024-07-21 21:33:12,035 - Ingest -WARNING -load_files method started....
2024-07-21 21:33:20,564 - Ingest -WARNING -dataframe of csv format successfully created....
2024-07-21 21:33:20,568 - root -INFO -displaying the dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string]
2024-07-21 21:33:21,019 - root -INFO -validating the dataframe...
2024-07-21 21:33:21,019 - Ingest -WARNING -here to count the records in the df_fact
2024-07-21 21:33:22,156 - Ingest -WARNING -Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-21 21:33:22,157 - root -INFO -Application done
2024-07-21 22:08:04,936 - root -INFO -i am in main method..
2024-07-21 22:08:04,937 - root -INFO -calling spark object
2024-07-21 22:08:04,937 - Create_spark -INFO -get_spark_object method started
2024-07-21 22:08:04,937 - Create_spark -INFO -master is local
2024-07-21 22:08:11,531 - Create_spark -INFO -Spark object created...
2024-07-21 22:08:11,531 - root -INFO -validating spark object..............
2024-07-21 22:08:11,531 - Validate -WARNING -started the get_current_date method...
2024-07-21 22:08:18,718 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 21))]
2024-07-21 22:08:18,719 - Validate -WARNING -Validation done, go frwd...
2024-07-21 22:08:18,719 - root -INFO -reading file which is of = parquet
2024-07-21 22:08:18,719 - Ingest -WARNING -load_files method started....
2024-07-21 22:08:19,808 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-07-21 22:08:19,818 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-07-21 22:08:21,332 - root -INFO -validating the dataframe...
2024-07-21 22:08:21,332 - Ingest -WARNING -here to count the records in the df_city
2024-07-21 22:08:22,672 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-21 22:08:22,672 - root -INFO -reading file which is of = csv
2024-07-21 22:08:22,673 - Ingest -WARNING -load_files method started....
2024-07-21 22:08:25,641 - Ingest -WARNING -dataframe of csv format successfully created....
2024-07-21 22:08:25,644 - root -INFO -displaying the dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string]
2024-07-21 22:08:26,069 - root -INFO -validating the dataframe...
2024-07-21 22:08:26,069 - Ingest -WARNING -here to count the records in the df_fact
2024-07-21 22:08:26,600 - Ingest -WARNING -Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 245492
2024-07-21 22:08:26,600 - root -INFO -Application done
2024-08-07 20:39:00,743 - root -INFO -i am in main method..
2024-08-07 20:39:00,747 - root -INFO -calling spark object
2024-08-07 20:39:00,749 - Create_spark -INFO -get_spark_object method started
2024-08-07 20:39:00,749 - Create_spark -INFO -master is local
2024-08-07 20:39:27,022 - Create_spark -INFO -Spark object created...
2024-08-07 20:39:27,023 - root -INFO -validating spark object..............
2024-08-07 20:39:27,023 - Validate -WARNING -started the get_current_date method...
2024-08-07 20:39:36,485 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 20:39:36,485 - Validate -WARNING -Validation done, go frwd...
2024-08-07 20:39:36,486 - root -INFO -reading file which is of = parquet
2024-08-07 20:39:36,486 - Ingest -WARNING -load_files method started....
2024-08-07 20:39:37,805 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-08-07 20:39:37,812 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-08-07 20:39:39,477 - root -INFO -validating the dataframe...
2024-08-07 20:39:39,477 - Ingest -WARNING -here to count the records in the df_city
2024-08-07 20:39:40,613 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 20:39:40,614 - root -INFO -reading file which is of = parquet
2024-08-07 20:39:40,614 - Ingest -WARNING -load_files method started....
2024-08-07 20:39:40,780 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-08-07 20:39:40,783 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-08-07 20:39:41,124 - root -INFO -validating the dataframe...
2024-08-07 20:39:41,125 - Ingest -WARNING -here to count the records in the df_fact
2024-08-07 20:39:41,543 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 20:39:41,543 - root -INFO -Application done
2024-08-07 20:43:07,810 - root -INFO -i am in main method..
2024-08-07 20:43:07,811 - root -INFO -calling spark object
2024-08-07 20:43:07,811 - Create_spark -INFO -get_spark_object method started
2024-08-07 20:43:07,811 - Create_spark -INFO -master is local
2024-08-07 20:43:15,292 - Create_spark -INFO -Spark object created...
2024-08-07 20:43:15,293 - root -INFO -validating spark object..............
2024-08-07 20:43:15,293 - Validate -WARNING -started the get_current_date method...
2024-08-07 20:43:22,811 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 20:43:22,811 - Validate -WARNING -Validation done, go frwd...
2024-08-07 20:43:22,812 - root -INFO -reading file which is of = parquet
2024-08-07 20:43:22,812 - Ingest -WARNING -load_files method started....
2024-08-07 20:43:23,983 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-08-07 20:43:23,990 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-08-07 20:43:25,851 - root -INFO -validating the dataframe...
2024-08-07 20:43:25,851 - Ingest -WARNING -here to count the records in the df_city
2024-08-07 20:43:26,955 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 20:43:26,955 - root -INFO -reading file which is of = parquet
2024-08-07 20:43:26,955 - Ingest -WARNING -load_files method started....
2024-08-07 20:43:27,187 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-08-07 20:43:27,192 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-08-07 20:43:27,469 - root -INFO -validating the dataframe...
2024-08-07 20:43:27,469 - Ingest -WARNING -here to count the records in the df_fact
2024-08-07 20:43:27,848 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 20:43:27,848 - root -INFO -Application done
2024-08-07 21:07:29,459 - root -INFO -i am in main method..
2024-08-07 21:07:29,459 - root -INFO -calling spark object
2024-08-07 21:07:29,459 - Create_spark -INFO -get_spark_object method started
2024-08-07 21:07:29,459 - Create_spark -INFO -master is local
2024-08-07 21:07:37,275 - Create_spark -INFO -Spark object created...
2024-08-07 21:07:37,275 - root -INFO -validating spark object..............
2024-08-07 21:07:37,276 - Validate -WARNING -started the get_current_date method...
2024-08-07 21:07:46,428 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 21:07:46,428 - Validate -WARNING -Validation done, go frwd...
2024-08-07 21:07:46,428 - root -INFO -reading file which is of = parquet
2024-08-07 21:07:46,429 - Ingest -WARNING -load_files method started....
2024-08-07 21:07:47,829 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-08-07 21:07:47,844 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-08-07 21:07:49,684 - root -INFO -validating the dataframe...
2024-08-07 21:07:49,684 - Ingest -WARNING -here to count the records in the df_city
2024-08-07 21:07:50,677 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 21:07:50,678 - root -INFO -reading file which is of = csv
2024-08-07 21:07:50,678 - Ingest -WARNING -load_files method started....
2024-08-07 21:08:01,374 - Ingest -WARNING -dataframe of csv format successfully created....
2024-08-07 21:08:01,377 - root -INFO -displaying the dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string]
2024-08-07 21:08:01,815 - root -INFO -validating the dataframe...
2024-08-07 21:08:01,815 - Ingest -WARNING -here to count the records in the df_fact
2024-08-07 21:08:03,328 - Ingest -WARNING -Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-07 21:08:03,328 - root -INFO -Application done
2024-08-07 21:08:16,252 - root -INFO -i am in main method..
2024-08-07 21:08:16,252 - root -INFO -calling spark object
2024-08-07 21:08:16,253 - Create_spark -INFO -get_spark_object method started
2024-08-07 21:08:16,253 - Create_spark -INFO -master is local
2024-08-07 21:08:23,831 - Create_spark -INFO -Spark object created...
2024-08-07 21:08:23,831 - root -INFO -validating spark object..............
2024-08-07 21:08:23,831 - Validate -WARNING -started the get_current_date method...
2024-08-07 21:08:31,727 - Validate -WARNING -validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 21:08:31,727 - Validate -WARNING -Validation done, go frwd...
2024-08-07 21:08:31,728 - root -INFO -reading file which is of = parquet
2024-08-07 21:08:31,728 - Ingest -WARNING -load_files method started....
2024-08-07 21:08:32,948 - Ingest -WARNING -dataframe of parquet format successfully created....
2024-08-07 21:08:32,956 - root -INFO -displaying the dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
2024-08-07 21:08:34,712 - root -INFO -validating the dataframe...
2024-08-07 21:08:34,712 - Ingest -WARNING -here to count the records in the df_city
2024-08-07 21:08:35,762 - Ingest -WARNING -Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 21:08:35,763 - root -INFO -reading file which is of = parquet
2024-08-07 21:08:35,763 - Ingest -WARNING -load_files method started....
